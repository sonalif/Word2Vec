{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "### Word embeddings\n",
    "Build embeddings with a keras implementation where the embedding vector is of length 50, 150 and 300. Use the Alice in Wonderland text book for training.\n",
    "1. Using the Skipgram model\n",
    "2. Using CBOW model\n",
    "3. Analyze the different word embeddings\n",
    "    - Implement your own function to perform the analogy task with. Do not use existing libraries for this task such as Gensim. Your function should be able to answer whether an anaology as in the example given in the pdf-file is true.\n",
    "    - Compare the performance on the analogy task between the word embeddings\n",
    "    - Visualize your results and interpret your results\n",
    "4. Discuss:\n",
    "    - What are the main advantages of CBOW and Skipgram?\n",
    "    - What are the main drawbacks of CBOW and Skipgram?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import gensim\n",
    "np.random.seed(13) #TODO Check if this is used for sgd\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Reshape, Lambda\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.preprocessing import sequence\n",
    "from keras import optimizers\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors as nn\n",
    "from itertools import islice\n",
    "from matplotlib import pylab\n",
    "import plotly.offline as plt\n",
    "import plotly.graph_objs as go\n",
    "plt.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT Modify the lines in this cell\n",
    "path = 'alice.txt'\n",
    "corpus = open(path).readlines()\n",
    "\n",
    "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"'\", lower=True)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "corpus = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "nb_samples = sum(len(s) for s in corpus)\n",
    "V = len(tokenizer.word_index) + 1\n",
    "# Is this something they need to change?\n",
    "dim1 = 50\n",
    "dim2= 150\n",
    "dim3 = 300\n",
    "window_size = 2\n",
    "window_size_corpus = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for skipgram\n",
    "def generate_data_skipgram(corpus, window_size, V):\n",
    "    #TODO Implement here\n",
    "    #TODO Finish the implementation of the method that prepares the training data for a skipgram model \n",
    "    maxlen = window_size*2\n",
    "    all_in = []\n",
    "    all_out = []\n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            p = index - window_size\n",
    "            n = index + window_size +1\n",
    "            \n",
    "            in_words = []\n",
    "            labels = []\n",
    "            for i in range(p,n):\n",
    "                if i != index and 0 <= i <L:\n",
    "                    #Add the input word\n",
    "                    #in_words.append(word)\n",
    "                    all_in.append(word)\n",
    "                    #add one of the context words\n",
    "                    all_out.append(np_utils.to_categorical(words[i], V))\n",
    "    \n",
    "    return (np.array(all_in),np.array(all_out))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data\n",
    "x,y = generate_data_skipgram(corpus,window_size,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create skipgram model\n",
    "skipgram1 = Sequential()\n",
    "#TODO Finish the model implementation\n",
    "skipgram1.add(Embedding(input_dim=V, output_dim=dim1, embeddings_initializer = 'glorot_uniform', input_length=1))\n",
    "skipgram1.add(Reshape((dim1,)))\n",
    "skipgram1.add(Dense(input_dim=dim1, units=V, kernel_initializer='uniform',activation='softmax'))\n",
    "skipgram1.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=0.01), metrics=['accuracy'])\n",
    "\n",
    "skipgram2 = Sequential()\n",
    "#TODO Finish the model implementation\n",
    "skipgram2.add(Embedding(input_dim=V, output_dim=dim2, embeddings_initializer = 'glorot_uniform', input_length=1))\n",
    "skipgram2.add(Reshape((dim2,)))\n",
    "skipgram2.add(Dense(input_dim=dim2, units=V, kernel_initializer='uniform',activation='softmax'))\n",
    "skipgram2.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=0.01), metrics=['accuracy'])\n",
    "\n",
    "skipgram3 = Sequential()\n",
    "#TODO Finish the model implementation\n",
    "skipgram3.add(Embedding(input_dim=V, output_dim=dim3, embeddings_initializer = 'glorot_uniform', input_length=1))\n",
    "skipgram3.add(Reshape((dim3,)))\n",
    "skipgram3.add(Dense(input_dim=dim3, units=V, kernel_initializer='uniform',activation='softmax'))\n",
    "skipgram3.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=0.01), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKIPGRAM MODEL:\n",
      "Embedding vector length = 50\n",
      "Epoch 1/50\n",
      "94556/94556 [==============================] - 11s 113us/step - loss: 6.0784 - acc: 0.0741\n",
      "Epoch 2/50\n",
      "94556/94556 [==============================] - 10s 104us/step - loss: 5.7882 - acc: 0.0869\n",
      "Epoch 3/50\n",
      "94556/94556 [==============================] - 10s 105us/step - loss: 5.8202 - acc: 0.0913\n",
      "Epoch 4/50\n",
      "94556/94556 [==============================] - 10s 105us/step - loss: 5.8245 - acc: 0.0949\n",
      "Epoch 5/50\n",
      "94556/94556 [==============================] - 10s 104us/step - loss: 5.8146 - acc: 0.0969\n",
      "Epoch 6/50\n",
      "94556/94556 [==============================] - 10s 109us/step - loss: 5.8094 - acc: 0.0983\n",
      "Epoch 7/50\n",
      "94556/94556 [==============================] - 10s 105us/step - loss: 5.8010 - acc: 0.0987\n",
      "Epoch 8/50\n",
      "94556/94556 [==============================] - 10s 105us/step - loss: 5.7907 - acc: 0.0991\n",
      "Epoch 9/50\n",
      "94556/94556 [==============================] - 10s 104us/step - loss: 5.7901 - acc: 0.1001\n",
      "Epoch 10/50\n",
      "94556/94556 [==============================] - 10s 105us/step - loss: 5.7848 - acc: 0.0993\n",
      "Epoch 11/50\n",
      "94556/94556 [==============================] - 10s 104us/step - loss: 5.7810 - acc: 0.1001\n",
      "Epoch 12/50\n",
      "94556/94556 [==============================] - 10s 105us/step - loss: 5.7806 - acc: 0.0998\n",
      "Epoch 13/50\n",
      "94556/94556 [==============================] - 10s 106us/step - loss: 5.7847 - acc: 0.0995\n",
      "Epoch 14/50\n",
      "94556/94556 [==============================] - 10s 107us/step - loss: 5.7810 - acc: 0.0994\n",
      "Epoch 15/50\n",
      "94556/94556 [==============================] - 10s 105us/step - loss: 5.7809 - acc: 0.0989\n",
      "Epoch 16/50\n",
      "94556/94556 [==============================] - 10s 105us/step - loss: 5.7750 - acc: 0.0989\n",
      "Epoch 17/50\n",
      "94556/94556 [==============================] - 10s 110us/step - loss: 5.7759 - acc: 0.0995\n",
      "Epoch 18/50\n",
      "94556/94556 [==============================] - 11s 119us/step - loss: 5.7800 - acc: 0.0998\n",
      "Epoch 19/50\n",
      "94556/94556 [==============================] - 10s 101us/step - loss: 5.7808 - acc: 0.0982\n",
      "Epoch 20/50\n",
      "94556/94556 [==============================] - 10s 108us/step - loss: 5.7773 - acc: 0.1000\n",
      "Epoch 21/50\n",
      "94556/94556 [==============================] - 9s 100us/step - loss: 5.7737 - acc: 0.0987\n",
      "Epoch 22/50\n",
      "94556/94556 [==============================] - 10s 111us/step - loss: 5.7754 - acc: 0.1001\n",
      "Epoch 23/50\n",
      "94556/94556 [==============================] - 10s 103us/step - loss: 5.7746 - acc: 0.0995\n",
      "Epoch 24/50\n",
      "94556/94556 [==============================] - 10s 106us/step - loss: 5.7787 - acc: 0.0988\n",
      "Epoch 25/50\n",
      "94556/94556 [==============================] - 10s 111us/step - loss: 5.7776 - acc: 0.0987\n",
      "Epoch 26/50\n",
      "94556/94556 [==============================] - 10s 102us/step - loss: 5.7809 - acc: 0.0977\n",
      "Epoch 27/50\n",
      "94556/94556 [==============================] - 9s 100us/step - loss: 5.7780 - acc: 0.0992\n",
      "Epoch 28/50\n",
      "94556/94556 [==============================] - 10s 101us/step - loss: 5.7793 - acc: 0.0989\n",
      "Epoch 29/50\n",
      "94556/94556 [==============================] - 10s 102us/step - loss: 5.7792 - acc: 0.0986\n",
      "Epoch 30/50\n",
      "94556/94556 [==============================] - 10s 104us/step - loss: 5.7779 - acc: 0.0992\n",
      "Epoch 31/50\n",
      "94556/94556 [==============================] - 9s 100us/step - loss: 5.7785 - acc: 0.0986\n",
      "Epoch 32/50\n",
      "94556/94556 [==============================] - 9s 100us/step - loss: 5.7765 - acc: 0.0982\n",
      "Epoch 33/50\n",
      "94556/94556 [==============================] - 10s 102us/step - loss: 5.7779 - acc: 0.0980\n",
      "Epoch 34/50\n",
      "94556/94556 [==============================] - 10s 103us/step - loss: 5.7804 - acc: 0.0989\n",
      "Epoch 35/50\n",
      "94556/94556 [==============================] - 9s 100us/step - loss: 5.7821 - acc: 0.0991\n",
      "Epoch 36/50\n",
      "94556/94556 [==============================] - 10s 101us/step - loss: 5.7775 - acc: 0.0994\n",
      "Epoch 37/50\n",
      "94556/94556 [==============================] - 10s 101us/step - loss: 5.7827 - acc: 0.0975\n",
      "Epoch 38/50\n",
      "94556/94556 [==============================] - 10s 101us/step - loss: 5.7772 - acc: 0.0993\n",
      "Epoch 39/50\n",
      "94556/94556 [==============================] - 9s 100us/step - loss: 5.7806 - acc: 0.0971\n",
      "Epoch 40/50\n",
      "94556/94556 [==============================] - 10s 101us/step - loss: 5.7798 - acc: 0.0991\n",
      "Epoch 41/50\n",
      "94556/94556 [==============================] - 10s 104us/step - loss: 5.7801 - acc: 0.0975\n",
      "Epoch 42/50\n",
      "94556/94556 [==============================] - 10s 101us/step - loss: 5.7849 - acc: 0.0980\n",
      "Epoch 43/50\n",
      "94556/94556 [==============================] - 10s 101us/step - loss: 5.7835 - acc: 0.0975\n",
      "Epoch 44/50\n",
      "94556/94556 [==============================] - 10s 101us/step - loss: 5.7846 - acc: 0.0970\n",
      "Epoch 45/50\n",
      "94556/94556 [==============================] - 9s 100us/step - loss: 5.7866 - acc: 0.0978\n",
      "Epoch 46/50\n",
      "94556/94556 [==============================] - 9s 99us/step - loss: 5.7822 - acc: 0.0985\n",
      "Epoch 47/50\n",
      "94556/94556 [==============================] - 9s 100us/step - loss: 5.7862 - acc: 0.0974\n",
      "Epoch 48/50\n",
      "94556/94556 [==============================] - 10s 101us/step - loss: 5.7855 - acc: 0.0985\n",
      "Epoch 49/50\n",
      "94556/94556 [==============================] - 9s 100us/step - loss: 5.7848 - acc: 0.0972\n",
      "Epoch 50/50\n",
      "94556/94556 [==============================] - 9s 100us/step - loss: 5.7841 - acc: 0.0979\n",
      "  \n",
      "Embedding vector length = 150\n",
      "Epoch 1/50\n",
      "94556/94556 [==============================] - 19s 199us/step - loss: 6.0793 - acc: 0.0768\n",
      "Epoch 2/50\n",
      "94556/94556 [==============================] - 19s 203us/step - loss: 5.7148 - acc: 0.0848\n",
      "Epoch 3/50\n",
      "94556/94556 [==============================] - 16s 166us/step - loss: 5.7119 - acc: 0.08871\n",
      "Epoch 4/50\n",
      "94556/94556 [==============================] - 15s 164us/step - loss: 5.6962 - acc: 0.0908\n",
      "Epoch 5/50\n",
      "94556/94556 [==============================] - 15s 162us/step - loss: 5.6738 - acc: 0.0904\n",
      "Epoch 6/50\n",
      "94556/94556 [==============================] - 15s 163us/step - loss: 5.6540 - acc: 0.0893\n",
      "Epoch 7/50\n",
      "94556/94556 [==============================] - 15s 163us/step - loss: 5.6370 - acc: 0.0896\n",
      "Epoch 8/50\n",
      "94556/94556 [==============================] - 15s 164us/step - loss: 5.6303 - acc: 0.0884\n",
      "Epoch 9/50\n",
      "94556/94556 [==============================] - 15s 163us/step - loss: 5.6285 - acc: 0.08891s - l\n",
      "Epoch 10/50\n",
      "94556/94556 [==============================] - 15s 164us/step - loss: 5.6274 - acc: 0.0890\n",
      "Epoch 11/50\n",
      "94556/94556 [==============================] - 16s 166us/step - loss: 5.6254 - acc: 0.0874\n",
      "Epoch 12/50\n",
      "94556/94556 [==============================] - 16s 168us/step - loss: 5.6320 - acc: 0.0880\n",
      "Epoch 13/50\n",
      "94556/94556 [==============================] - 16s 165us/step - loss: 5.6307 - acc: 0.0876\n",
      "Epoch 14/50\n",
      "94556/94556 [==============================] - 16s 166us/step - loss: 5.6386 - acc: 0.0890\n",
      "Epoch 15/50\n",
      "94556/94556 [==============================] - 17s 182us/step - loss: 5.6386 - acc: 0.0891\n",
      "Epoch 16/50\n",
      "94556/94556 [==============================] - 17s 183us/step - loss: 5.6439 - acc: 0.0867\n",
      "Epoch 17/50\n",
      "94556/94556 [==============================] - 17s 184us/step - loss: 5.6463 - acc: 0.0879\n",
      "Epoch 18/50\n",
      "94556/94556 [==============================] - 17s 180us/step - loss: 5.6459 - acc: 0.0882\n",
      "Epoch 19/50\n",
      "94556/94556 [==============================] - 17s 181us/step - loss: 5.6522 - acc: 0.0879\n",
      "Epoch 20/50\n",
      "94556/94556 [==============================] - 18s 187us/step - loss: 5.6528 - acc: 0.0889\n",
      "Epoch 21/50\n",
      "94556/94556 [==============================] - 17s 185us/step - loss: 5.6599 - acc: 0.0861\n",
      "Epoch 22/50\n",
      "94556/94556 [==============================] - 17s 184us/step - loss: 5.6603 - acc: 0.0868\n",
      "Epoch 23/50\n",
      "94556/94556 [==============================] - 17s 182us/step - loss: 5.6631 - acc: 0.0863\n",
      "Epoch 24/50\n",
      "94556/94556 [==============================] - 17s 184us/step - loss: 5.6615 - acc: 0.0866\n",
      "Epoch 25/50\n",
      "94556/94556 [==============================] - 17s 184us/step - loss: 5.6670 - acc: 0.0866\n",
      "Epoch 26/50\n",
      "94556/94556 [==============================] - 17s 182us/step - loss: 5.6673 - acc: 0.0859\n",
      "Epoch 27/50\n",
      "94556/94556 [==============================] - 17s 184us/step - loss: 5.6772 - acc: 0.0853\n",
      "Epoch 28/50\n",
      "94556/94556 [==============================] - 17s 183us/step - loss: 5.6783 - acc: 0.08641s\n",
      "Epoch 29/50\n",
      "94556/94556 [==============================] - 17s 184us/step - loss: 5.6777 - acc: 0.0866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50\n",
      "94556/94556 [==============================] - 17s 182us/step - loss: 5.6792 - acc: 0.0868\n",
      "Epoch 31/50\n",
      "94556/94556 [==============================] - 17s 183us/step - loss: 5.6793 - acc: 0.0862\n",
      "Epoch 32/50\n",
      "94556/94556 [==============================] - 18s 186us/step - loss: 5.6854 - acc: 0.0859\n",
      "Epoch 33/50\n",
      "94556/94556 [==============================] - 20s 216us/step - loss: 5.6883 - acc: 0.0868\n",
      "Epoch 34/50\n",
      "94556/94556 [==============================] - 19s 200us/step - loss: 5.6927 - acc: 0.08601s - loss:\n",
      "Epoch 35/50\n",
      "94556/94556 [==============================] - 20s 216us/step - loss: 5.6925 - acc: 0.0868\n",
      "Epoch 36/50\n",
      "94556/94556 [==============================] - 20s 206us/step - loss: 5.6930 - acc: 0.0852\n",
      "Epoch 37/50\n",
      "94556/94556 [==============================] - 18s 186us/step - loss: 5.6923 - acc: 0.0862\n",
      "Epoch 38/50\n",
      "94556/94556 [==============================] - 18s 185us/step - loss: 5.6930 - acc: 0.0850\n",
      "Epoch 39/50\n",
      "94556/94556 [==============================] - 17s 183us/step - loss: 5.6967 - acc: 0.0853\n",
      "Epoch 40/50\n",
      "94556/94556 [==============================] - 18s 186us/step - loss: 5.6955 - acc: 0.0850\n",
      "Epoch 41/50\n",
      "94556/94556 [==============================] - 18s 188us/step - loss: 5.6983 - acc: 0.08500s - loss: 5.6949 - ac\n",
      "Epoch 42/50\n",
      "94556/94556 [==============================] - 17s 183us/step - loss: 5.7019 - acc: 0.0850\n",
      "Epoch 43/50\n",
      "94556/94556 [==============================] - 17s 184us/step - loss: 5.7027 - acc: 0.0849\n",
      "Epoch 44/50\n",
      "94556/94556 [==============================] - 18s 187us/step - loss: 5.7011 - acc: 0.0864\n",
      "Epoch 45/50\n",
      "94556/94556 [==============================] - 17s 183us/step - loss: 5.7093 - acc: 0.0848\n",
      "Epoch 46/50\n",
      "94556/94556 [==============================] - 17s 183us/step - loss: 5.7059 - acc: 0.0858\n",
      "Epoch 47/50\n",
      "94556/94556 [==============================] - 17s 183us/step - loss: 5.7108 - acc: 0.0862\n",
      "Epoch 48/50\n",
      "94556/94556 [==============================] - 17s 182us/step - loss: 5.7176 - acc: 0.0848\n",
      "Epoch 49/50\n",
      "94556/94556 [==============================] - 17s 182us/step - loss: 5.7134 - acc: 0.0859\n",
      "Epoch 50/50\n",
      "94556/94556 [==============================] - 17s 182us/step - loss: 5.7170 - acc: 0.0845\n",
      "  \n",
      "Embedding vector length = 300\n",
      "Epoch 1/50\n",
      "94556/94556 [==============================] - 27s 287us/step - loss: 6.0909 - acc: 0.0750\n",
      "Epoch 2/50\n",
      "94556/94556 [==============================] - 27s 283us/step - loss: 5.6943 - acc: 0.0814\n",
      "Epoch 3/50\n",
      "94556/94556 [==============================] - 27s 284us/step - loss: 5.6989 - acc: 0.0837\n",
      "Epoch 4/50\n",
      "94556/94556 [==============================] - 27s 283us/step - loss: 5.6842 - acc: 0.0835\n",
      "Epoch 5/50\n",
      "94556/94556 [==============================] - 27s 284us/step - loss: 5.6699 - acc: 0.0822\n",
      "Epoch 6/50\n",
      "94556/94556 [==============================] - 27s 286us/step - loss: 5.6573 - acc: 0.0812\n",
      "Epoch 7/50\n",
      "94556/94556 [==============================] - 27s 283us/step - loss: 5.6543 - acc: 0.0808\n",
      "Epoch 8/50\n",
      "94556/94556 [==============================] - 27s 282us/step - loss: 5.6525 - acc: 0.0806\n",
      "Epoch 9/50\n",
      "94556/94556 [==============================] - 27s 282us/step - loss: 5.6519 - acc: 0.0809\n",
      "Epoch 10/50\n",
      "94556/94556 [==============================] - 27s 290us/step - loss: 5.6544 - acc: 0.0810\n",
      "Epoch 11/50\n",
      "94556/94556 [==============================] - 27s 283us/step - loss: 5.6582 - acc: 0.0801\n",
      "Epoch 12/50\n",
      "94556/94556 [==============================] - 27s 283us/step - loss: 5.6620 - acc: 0.0804\n",
      "Epoch 13/50\n",
      "94556/94556 [==============================] - 27s 282us/step - loss: 5.6607 - acc: 0.0812\n",
      "Epoch 14/50\n",
      "94556/94556 [==============================] - 27s 285us/step - loss: 5.6636 - acc: 0.0801\n",
      "Epoch 15/50\n",
      "94556/94556 [==============================] - 27s 285us/step - loss: 5.6678 - acc: 0.0793\n",
      "Epoch 16/50\n",
      "94556/94556 [==============================] - 27s 285us/step - loss: 5.6730 - acc: 0.0811\n",
      "Epoch 17/50\n",
      "94556/94556 [==============================] - 27s 284us/step - loss: 5.6761 - acc: 0.0791\n",
      "Epoch 18/50\n",
      "94556/94556 [==============================] - 27s 282us/step - loss: 5.6811 - acc: 0.0804\n",
      "Epoch 19/50\n",
      "94556/94556 [==============================] - 27s 283us/step - loss: 5.6832 - acc: 0.0792\n",
      "Epoch 20/50\n",
      "94556/94556 [==============================] - 27s 283us/step - loss: 5.6883 - acc: 0.0802\n",
      "Epoch 21/50\n",
      "94556/94556 [==============================] - 27s 282us/step - loss: 5.6880 - acc: 0.0798\n",
      "Epoch 22/50\n",
      "94556/94556 [==============================] - 27s 282us/step - loss: 5.6887 - acc: 0.0802\n",
      "Epoch 23/50\n",
      "94556/94556 [==============================] - 27s 290us/step - loss: 5.6955 - acc: 0.0798\n",
      "Epoch 24/50\n",
      "94556/94556 [==============================] - 26s 278us/step - loss: 5.6941 - acc: 0.0802\n",
      "Epoch 25/50\n",
      "94556/94556 [==============================] - 31s 325us/step - loss: 5.6992 - acc: 0.0789\n",
      "Epoch 26/50\n",
      "94556/94556 [==============================] - 23s 241us/step - loss: 5.7004 - acc: 0.0803\n",
      "Epoch 27/50\n",
      "94556/94556 [==============================] - 24s 251us/step - loss: 5.7012 - acc: 0.0807\n",
      "Epoch 28/50\n",
      "94556/94556 [==============================] - 24s 255us/step - loss: 5.7079 - acc: 0.0797\n",
      "Epoch 29/50\n",
      "94556/94556 [==============================] - 26s 277us/step - loss: 5.7091 - acc: 0.0809\n",
      "Epoch 30/50\n",
      "94556/94556 [==============================] - 23s 239us/step - loss: 5.7117 - acc: 0.0792\n",
      "Epoch 31/50\n",
      "94556/94556 [==============================] - 24s 253us/step - loss: 5.7193 - acc: 0.0796\n",
      "Epoch 32/50\n",
      "94556/94556 [==============================] - 23s 246us/step - loss: 5.7151 - acc: 0.0798\n",
      "Epoch 33/50\n",
      "94556/94556 [==============================] - 24s 249us/step - loss: 5.7192 - acc: 0.0802\n",
      "Epoch 34/50\n",
      "94556/94556 [==============================] - 24s 250us/step - loss: 5.7159 - acc: 0.0793\n",
      "Epoch 35/50\n",
      "94556/94556 [==============================] - 23s 246us/step - loss: 5.7205 - acc: 0.0801\n",
      "Epoch 36/50\n",
      "94556/94556 [==============================] - 30s 312us/step - loss: 5.7164 - acc: 0.0789\n",
      "Epoch 37/50\n",
      "94556/94556 [==============================] - 30s 322us/step - loss: 5.7254 - acc: 0.0789\n",
      "Epoch 38/50\n",
      "94556/94556 [==============================] - 24s 258us/step - loss: 5.7219 - acc: 0.0804\n",
      "Epoch 39/50\n",
      "94556/94556 [==============================] - 26s 275us/step - loss: 5.7267 - acc: 0.0796\n",
      "Epoch 40/50\n",
      "94556/94556 [==============================] - 26s 277us/step - loss: 5.7273 - acc: 0.0804\n",
      "Epoch 41/50\n",
      "94556/94556 [==============================] - 25s 259us/step - loss: 5.7294 - acc: 0.0795\n",
      "Epoch 42/50\n",
      "94556/94556 [==============================] - 26s 272us/step - loss: 5.7276 - acc: 0.0797\n",
      "Epoch 43/50\n",
      "94556/94556 [==============================] - 26s 276us/step - loss: 5.7300 - acc: 0.0811\n",
      "Epoch 44/50\n",
      "94556/94556 [==============================] - 26s 275us/step - loss: 5.7335 - acc: 0.0804\n",
      "Epoch 45/50\n",
      "94556/94556 [==============================] - 25s 263us/step - loss: 5.7314 - acc: 0.0800\n",
      "Epoch 46/50\n",
      "94556/94556 [==============================] - 23s 238us/step - loss: 5.7346 - acc: 0.07913s  - ETA: 1s - lo\n",
      "Epoch 47/50\n",
      "94556/94556 [==============================] - 22s 229us/step - loss: 5.7339 - acc: 0.0782\n",
      "Epoch 48/50\n",
      "94556/94556 [==============================] - 22s 231us/step - loss: 5.7377 - acc: 0.0797\n",
      "Epoch 49/50\n",
      "94556/94556 [==============================] - 22s 238us/step - loss: 5.7317 - acc: 0.0794\n",
      "Epoch 50/50\n",
      "94556/94556 [==============================] - 21s 227us/step - loss: 5.7321 - acc: 0.0789\n",
      "  \n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "print(\"SKIPGRAM MODEL:\")\n",
    "print(\"Embedding vector length = 50\")\n",
    "skipgram1.fit(x, y, batch_size=128, epochs=50, verbose=1)\n",
    "print(\"  \")\n",
    "print(\"Embedding vector length = 150\")\n",
    "skipgram2.fit(x, y, batch_size=128, epochs=50, verbose=1)\n",
    "print(\"  \")\n",
    "print(\"Embedding vector length = 300\")\n",
    "skipgram3.fit(x, y, batch_size=128, epochs=50, verbose=1)\n",
    "print(\"  \")\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for CBOW\n",
    "def generate_data_cbow(corpus, window_size, V):\n",
    "\n",
    "    #TODO Implement here\n",
    "    #TODO Finish the implementation of the method that prepares the training data for a skipgram model \n",
    "    maxlen = window_size*2\n",
    "    all_in = []\n",
    "    all_out = []\n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            p = index - window_size\n",
    "            n = index + window_size +1\n",
    "            \n",
    "            in_words = []\n",
    "            labels = []\n",
    "            for i in range(p,n):\n",
    "                if i != index and 0 <= i <L:\n",
    "                    #Add the input words\n",
    "                    all_in.append(words[i])\n",
    "                    #in_words.append(word)\n",
    "                    #add one of the context words\n",
    "                    all_out.append(np_utils.to_categorical(word, V))\n",
    "    #all_in = sequence.pad_sequences(all_in, maxlen=maxlen)\n",
    "    return (np.array(all_in),np.array(all_out))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,y1 = generate_data_cbow(corpus,window_size,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create cbow model\n",
    "cbow1 = Sequential()\n",
    "#TODO Finish the model implementation\n",
    "cbow1.add(Embedding(input_dim=V, output_dim=dim1, embeddings_initializer = 'glorot_uniform', input_length=1))\n",
    "cbow1.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim1,)))\n",
    "cbow1.add(Dense(input_dim=dim1, units=V, kernel_initializer='uniform',activation='softmax'))\n",
    "cbow1.compile(loss='categorical_crossentropy', optimizer='RMSprop')\n",
    "\n",
    "cbow2 = Sequential()\n",
    "#TODO Finish the model implementation\n",
    "cbow2.add(Embedding(input_dim=V, output_dim=dim2, embeddings_initializer = 'glorot_uniform', input_length=1))\n",
    "cbow2.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim2,)))\n",
    "cbow2.add(Dense(input_dim=dim2, units=V, kernel_initializer='uniform',activation='softmax'))\n",
    "cbow2.compile(loss='categorical_crossentropy', optimizer='RMSprop')\n",
    "\n",
    "cbow3 = Sequential()\n",
    "#TODO Finish the model implementation\n",
    "cbow3.add(Embedding(input_dim=V, output_dim=dim3, embeddings_initializer = 'glorot_uniform', input_length=1))\n",
    "cbow3.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim3,)))\n",
    "cbow3.add(Dense(input_dim=dim3, units=V, kernel_initializer='uniform',activation='softmax'))\n",
    "cbow3.compile(loss='categorical_crossentropy', optimizer='RMSprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW MODEL:\n",
      " \n",
      "Embedding vector length = 50\n",
      "Epoch 1/50\n",
      "94556/94556 [==============================] - 9s 93us/step - loss: 6.7307\n",
      "Epoch 2/50\n",
      "94556/94556 [==============================] - 8s 87us/step - loss: 6.0546\n",
      "Epoch 3/50\n",
      "94556/94556 [==============================] - 9s 90us/step - loss: 5.9449\n",
      "Epoch 4/50\n",
      "94556/94556 [==============================] - 8s 87us/step - loss: 5.8764\n",
      "Epoch 5/50\n",
      "94556/94556 [==============================] - 8s 89us/step - loss: 5.8272\n",
      "Epoch 6/50\n",
      "94556/94556 [==============================] - 8s 90us/step - loss: 5.7893\n",
      "Epoch 7/50\n",
      "94556/94556 [==============================] - 8s 87us/step - loss: 5.7581\n",
      "Epoch 8/50\n",
      "94556/94556 [==============================] - 8s 89us/step - loss: 5.7305\n",
      "Epoch 9/50\n",
      "94556/94556 [==============================] - 8s 87us/step - loss: 5.7074\n",
      "Epoch 10/50\n",
      "94556/94556 [==============================] - 8s 87us/step - loss: 5.6869\n",
      "Epoch 11/50\n",
      "94556/94556 [==============================] - 8s 87us/step - loss: 5.6672\n",
      "Epoch 12/50\n",
      "94556/94556 [==============================] - 8s 87us/step - loss: 5.6496\n",
      "Epoch 13/50\n",
      "94556/94556 [==============================] - 8s 87us/step - loss: 5.6344\n",
      "Epoch 14/50\n",
      "94556/94556 [==============================] - 8s 87us/step - loss: 5.6192\n",
      "Epoch 15/50\n",
      "94556/94556 [==============================] - 8s 87us/step - loss: 5.6073\n",
      "Epoch 16/50\n",
      "94556/94556 [==============================] - 8s 87us/step - loss: 5.5949\n",
      "Epoch 17/50\n",
      "94556/94556 [==============================] - 8s 88us/step - loss: 5.5843\n",
      "Epoch 18/50\n",
      "94556/94556 [==============================] - 8s 89us/step - loss: 5.5736\n",
      "Epoch 19/50\n",
      "94556/94556 [==============================] - 8s 87us/step - loss: 5.5657\n",
      "Epoch 20/50\n",
      "94556/94556 [==============================] - 8s 87us/step - loss: 5.5581\n",
      "Epoch 21/50\n",
      "94556/94556 [==============================] - 8s 88us/step - loss: 5.5524\n",
      "Epoch 22/50\n",
      "94556/94556 [==============================] - 8s 88us/step - loss: 5.5469\n",
      "Epoch 23/50\n",
      "94556/94556 [==============================] - 8s 87us/step - loss: 5.5421\n",
      "Epoch 24/50\n",
      "94556/94556 [==============================] - 8s 88us/step - loss: 5.5392\n",
      "Epoch 25/50\n",
      "94556/94556 [==============================] - 8s 88us/step - loss: 5.5376\n",
      "Epoch 26/50\n",
      "94556/94556 [==============================] - 8s 88us/step - loss: 5.5375\n",
      "Epoch 27/50\n",
      "94556/94556 [==============================] - 8s 87us/step - loss: 5.5437\n",
      "Epoch 28/50\n",
      "94556/94556 [==============================] - 8s 87us/step - loss: 5.5542\n",
      "Epoch 29/50\n",
      "94556/94556 [==============================] - 8s 90us/step - loss: 5.5691\n",
      "Epoch 30/50\n",
      "94556/94556 [==============================] - 8s 88us/step - loss: 5.5856\n",
      "Epoch 31/50\n",
      "94556/94556 [==============================] - 8s 88us/step - loss: 5.5998\n",
      "Epoch 32/50\n",
      "94556/94556 [==============================] - 8s 88us/step - loss: 5.6146\n",
      "Epoch 33/50\n",
      "94556/94556 [==============================] - 8s 88us/step - loss: 5.6250\n",
      "Epoch 34/50\n",
      "94556/94556 [==============================] - 8s 88us/step - loss: 5.6357\n",
      "Epoch 35/50\n",
      "94556/94556 [==============================] - 8s 89us/step - loss: 5.6414\n",
      "Epoch 36/50\n",
      "94556/94556 [==============================] - 8s 88us/step - loss: 5.6443\n",
      "Epoch 37/50\n",
      "94556/94556 [==============================] - 8s 88us/step - loss: 5.6449\n",
      "Epoch 38/50\n",
      "94556/94556 [==============================] - 8s 88us/step - loss: 5.6436\n",
      "Epoch 39/50\n",
      "94556/94556 [==============================] - 8s 88us/step - loss: 5.6413\n",
      "Epoch 40/50\n",
      "94556/94556 [==============================] - 8s 88us/step - loss: 5.6422\n",
      "Epoch 41/50\n",
      "94556/94556 [==============================] - 8s 89us/step - loss: 5.6396\n",
      "Epoch 42/50\n",
      "94556/94556 [==============================] - 8s 88us/step - loss: 5.6353\n",
      "Epoch 43/50\n",
      "94556/94556 [==============================] - 8s 89us/step - loss: 5.6350\n",
      "Epoch 44/50\n",
      "94556/94556 [==============================] - 9s 91us/step - loss: 5.6332\n",
      "Epoch 45/50\n",
      "94556/94556 [==============================] - 8s 88us/step - loss: 5.6317\n",
      "Epoch 46/50\n",
      "94556/94556 [==============================] - 8s 88us/step - loss: 5.6272\n",
      "Epoch 47/50\n",
      "94556/94556 [==============================] - 8s 88us/step - loss: 5.6248\n",
      "Epoch 48/50\n",
      "94556/94556 [==============================] - 8s 88us/step - loss: 5.6244\n",
      "Epoch 49/50\n",
      "94556/94556 [==============================] - 8s 88us/step - loss: 5.6221\n",
      "Epoch 50/50\n",
      "94556/94556 [==============================] - 8s 88us/step - loss: 5.6225\n",
      "  \n",
      "Embedding vector length = 150\n",
      "Epoch 1/50\n",
      "94556/94556 [==============================] - 16s 169us/step - loss: 6.5226\n",
      "Epoch 2/50\n",
      "94556/94556 [==============================] - 16s 165us/step - loss: 5.9392\n",
      "Epoch 3/50\n",
      "94556/94556 [==============================] - 16s 166us/step - loss: 5.8219\n",
      "Epoch 4/50\n",
      "94556/94556 [==============================] - 16s 167us/step - loss: 5.7540\n",
      "Epoch 5/50\n",
      "94556/94556 [==============================] - 16s 165us/step - loss: 5.7046\n",
      "Epoch 6/50\n",
      "94556/94556 [==============================] - 16s 168us/step - loss: 5.6637\n",
      "Epoch 7/50\n",
      "94556/94556 [==============================] - 16s 165us/step - loss: 5.6294\n",
      "Epoch 8/50\n",
      "94556/94556 [==============================] - 17s 180us/step - loss: 5.5956\n",
      "Epoch 9/50\n",
      "94556/94556 [==============================] - 18s 187us/step - loss: 5.56580s - loss: 5.5\n",
      "Epoch 10/50\n",
      "94556/94556 [==============================] - 14s 145us/step - loss: 5.5390\n",
      "Epoch 11/50\n",
      "94556/94556 [==============================] - 13s 142us/step - loss: 5.5122\n",
      "Epoch 12/50\n",
      "94556/94556 [==============================] - 14s 143us/step - loss: 5.4885\n",
      "Epoch 13/50\n",
      "94556/94556 [==============================] - 13s 142us/step - loss: 5.4648\n",
      "Epoch 14/50\n",
      "94556/94556 [==============================] - 16s 174us/step - loss: 5.4428\n",
      "Epoch 15/50\n",
      "94556/94556 [==============================] - 16s 174us/step - loss: 5.4233\n",
      "Epoch 16/50\n",
      "94556/94556 [==============================] - 16s 172us/step - loss: 5.4031\n",
      "Epoch 17/50\n",
      "94556/94556 [==============================] - 17s 175us/step - loss: 5.3861\n",
      "Epoch 18/50\n",
      "94556/94556 [==============================] - 16s 171us/step - loss: 5.3703\n",
      "Epoch 19/50\n",
      "94556/94556 [==============================] - 16s 169us/step - loss: 5.3564\n",
      "Epoch 20/50\n",
      "94556/94556 [==============================] - 16s 168us/step - loss: 5.3423\n",
      "Epoch 21/50\n",
      "94556/94556 [==============================] - 16s 171us/step - loss: 5.3297\n",
      "Epoch 22/50\n",
      "94556/94556 [==============================] - 16s 169us/step - loss: 5.3185\n",
      "Epoch 23/50\n",
      "94556/94556 [==============================] - 16s 168us/step - loss: 5.30760s - loss: 5.306\n",
      "Epoch 24/50\n",
      "94556/94556 [==============================] - 16s 166us/step - loss: 5.3001\n",
      "Epoch 25/50\n",
      "94556/94556 [==============================] - 16s 167us/step - loss: 5.2937\n",
      "Epoch 26/50\n",
      "94556/94556 [==============================] - 16s 166us/step - loss: 5.2875\n",
      "Epoch 27/50\n",
      "94556/94556 [==============================] - 16s 169us/step - loss: 5.2832\n",
      "Epoch 28/50\n",
      "94556/94556 [==============================] - 16s 167us/step - loss: 5.2784\n",
      "Epoch 29/50\n",
      "94556/94556 [==============================] - 16s 169us/step - loss: 5.2751\n",
      "Epoch 30/50\n",
      "94556/94556 [==============================] - 16s 166us/step - loss: 5.2734\n",
      "Epoch 31/50\n",
      "94556/94556 [==============================] - 16s 166us/step - loss: 5.2735\n",
      "Epoch 32/50\n",
      "94556/94556 [==============================] - 16s 166us/step - loss: 5.2748\n",
      "Epoch 33/50\n",
      "94556/94556 [==============================] - 16s 170us/step - loss: 5.2762\n",
      "Epoch 34/50\n",
      "94556/94556 [==============================] - 16s 172us/step - loss: 5.2796\n",
      "Epoch 35/50\n",
      "94556/94556 [==============================] - 16s 173us/step - loss: 5.2814\n",
      "Epoch 36/50\n",
      "94556/94556 [==============================] - 16s 174us/step - loss: 5.2878\n",
      "Epoch 37/50\n",
      "94556/94556 [==============================] - 16s 166us/step - loss: 5.2910\n",
      "Epoch 38/50\n",
      "94556/94556 [==============================] - 16s 166us/step - loss: 5.29490s - \n",
      "Epoch 39/50\n",
      "94556/94556 [==============================] - 16s 168us/step - loss: 5.2978\n",
      "Epoch 40/50\n",
      "94556/94556 [==============================] - 16s 168us/step - loss: 5.3062\n",
      "Epoch 41/50\n",
      "94556/94556 [==============================] - 16s 168us/step - loss: 5.3160\n",
      "Epoch 42/50\n",
      "94556/94556 [==============================] - 16s 171us/step - loss: 5.3269\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94556/94556 [==============================] - 15s 156us/step - loss: 5.3353\n",
      "Epoch 44/50\n",
      "94556/94556 [==============================] - 15s 156us/step - loss: 5.3422\n",
      "Epoch 45/50\n",
      "94556/94556 [==============================] - 15s 161us/step - loss: 5.34720s - loss:\n",
      "Epoch 46/50\n",
      "94556/94556 [==============================] - 16s 167us/step - loss: 5.3488\n",
      "Epoch 47/50\n",
      "94556/94556 [==============================] - 16s 170us/step - loss: 5.3494\n",
      "Epoch 48/50\n",
      "94556/94556 [==============================] - 17s 184us/step - loss: 5.3467\n",
      "Epoch 49/50\n",
      "94556/94556 [==============================] - 15s 161us/step - loss: 5.3464\n",
      "Epoch 50/50\n",
      "94556/94556 [==============================] - 15s 159us/step - loss: 5.3432\n",
      "  \n",
      "Embedding vector length = 300\n",
      "Epoch 1/50\n",
      "94556/94556 [==============================] - 24s 252us/step - loss: 6.4039\n",
      "Epoch 2/50\n",
      "94556/94556 [==============================] - 23s 248us/step - loss: 5.8591\n",
      "Epoch 3/50\n",
      "94556/94556 [==============================] - 24s 249us/step - loss: 5.7497\n",
      "Epoch 4/50\n",
      "94556/94556 [==============================] - 24s 249us/step - loss: 5.6821\n",
      "Epoch 5/50\n",
      "94556/94556 [==============================] - 24s 249us/step - loss: 5.6301\n",
      "Epoch 6/50\n",
      "94556/94556 [==============================] - 24s 252us/step - loss: 5.5823\n",
      "Epoch 7/50\n",
      "94556/94556 [==============================] - 24s 249us/step - loss: 5.5356\n",
      "Epoch 8/50\n",
      "94556/94556 [==============================] - 24s 255us/step - loss: 5.4949\n",
      "Epoch 9/50\n",
      "94556/94556 [==============================] - 29s 312us/step - loss: 5.4565\n",
      "Epoch 10/50\n",
      "94556/94556 [==============================] - 27s 288us/step - loss: 5.4195\n",
      "Epoch 11/50\n",
      "94556/94556 [==============================] - 27s 289us/step - loss: 5.3879\n",
      "Epoch 12/50\n",
      "94556/94556 [==============================] - 27s 283us/step - loss: 5.3575\n",
      "Epoch 13/50\n",
      "94556/94556 [==============================] - 27s 285us/step - loss: 5.3302\n",
      "Epoch 14/50\n",
      "94556/94556 [==============================] - 26s 280us/step - loss: 5.3054\n",
      "Epoch 15/50\n",
      "94556/94556 [==============================] - 27s 281us/step - loss: 5.2810\n",
      "Epoch 16/50\n",
      "94556/94556 [==============================] - 26s 279us/step - loss: 5.2608\n",
      "Epoch 17/50\n",
      "94556/94556 [==============================] - 26s 276us/step - loss: 5.2400\n",
      "Epoch 18/50\n",
      "94556/94556 [==============================] - 26s 277us/step - loss: 5.2235\n",
      "Epoch 19/50\n",
      "94556/94556 [==============================] - 26s 277us/step - loss: 5.2079\n",
      "Epoch 20/50\n",
      "94556/94556 [==============================] - 26s 277us/step - loss: 5.1932\n",
      "Epoch 21/50\n",
      "94556/94556 [==============================] - 25s 269us/step - loss: 5.1804\n",
      "Epoch 22/50\n",
      "94556/94556 [==============================] - 23s 248us/step - loss: 5.1679\n",
      "Epoch 23/50\n",
      "94556/94556 [==============================] - 23s 247us/step - loss: 5.1581\n",
      "Epoch 24/50\n",
      "94556/94556 [==============================] - 23s 247us/step - loss: 5.1542\n",
      "Epoch 25/50\n",
      "94556/94556 [==============================] - 23s 247us/step - loss: 5.1524\n",
      "Epoch 26/50\n",
      "94556/94556 [==============================] - 23s 246us/step - loss: 5.1521\n",
      "Epoch 27/50\n",
      "94556/94556 [==============================] - 23s 247us/step - loss: 5.1488\n",
      "Epoch 28/50\n",
      "94556/94556 [==============================] - 23s 243us/step - loss: 5.1463\n",
      "Epoch 29/50\n",
      "94556/94556 [==============================] - 23s 241us/step - loss: 5.1442\n",
      "Epoch 30/50\n",
      "94556/94556 [==============================] - 23s 240us/step - loss: 5.1388\n",
      "Epoch 31/50\n",
      "94556/94556 [==============================] - 23s 245us/step - loss: 5.1390\n",
      "Epoch 32/50\n",
      "94556/94556 [==============================] - 23s 248us/step - loss: 5.1413\n",
      "Epoch 33/50\n",
      "94556/94556 [==============================] - 23s 248us/step - loss: 5.1465\n",
      "Epoch 34/50\n",
      "94556/94556 [==============================] - 23s 247us/step - loss: 5.15160s - loss: 5.\n",
      "Epoch 35/50\n",
      "94556/94556 [==============================] - 27s 288us/step - loss: 5.1569\n",
      "Epoch 36/50\n",
      "94556/94556 [==============================] - 41s 432us/step - loss: 5.1636\n",
      "Epoch 37/50\n",
      "94556/94556 [==============================] - 41s 435us/step - loss: 5.16850s - loss: 5.16\n",
      "Epoch 38/50\n",
      "94556/94556 [==============================] - 42s 442us/step - loss: 5.1736\n",
      "Epoch 39/50\n",
      "94556/94556 [==============================] - 41s 438us/step - loss: 5.1806\n",
      "Epoch 40/50\n",
      "94556/94556 [==============================] - 41s 436us/step - loss: 5.1885\n",
      "Epoch 41/50\n",
      "94556/94556 [==============================] - 41s 435us/step - loss: 5.1897\n",
      "Epoch 42/50\n",
      "94556/94556 [==============================] - 41s 437us/step - loss: 5.1903\n",
      "Epoch 43/50\n",
      "94556/94556 [==============================] - 41s 435us/step - loss: 5.1966\n",
      "Epoch 44/50\n",
      "94556/94556 [==============================] - 41s 435us/step - loss: 5.1971\n",
      "Epoch 45/50\n",
      "94556/94556 [==============================] - 41s 436us/step - loss: 5.1980\n",
      "Epoch 46/50\n",
      "94556/94556 [==============================] - 41s 433us/step - loss: 5.1985\n",
      "Epoch 47/50\n",
      "94556/94556 [==============================] - 41s 438us/step - loss: 5.20192s - - ETA: 2s\n",
      "Epoch 48/50\n",
      "94556/94556 [==============================] - 42s 444us/step - loss: 5.2001\n",
      "Epoch 49/50\n",
      "94556/94556 [==============================] - 42s 443us/step - loss: 5.1983\n",
      "Epoch 50/50\n",
      "94556/94556 [==============================] - 42s 440us/step - loss: 5.1991\n",
      "  \n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#train cbow model\n",
    "print(\"CBOW MODEL:\")\n",
    "print(\" \")\n",
    "print(\"Embedding vector length = 50\")\n",
    "cbow1.fit(x1, y1, batch_size=128, epochs=50, verbose=1)\n",
    "print(\"  \")\n",
    "print(\"Embedding vector length = 150\")\n",
    "cbow2.fit(x1, y1, batch_size=128, epochs=50, verbose=1)\n",
    "print(\"  \")\n",
    "print(\"Embedding vector length = 300\")\n",
    "cbow3.fit(x1, y1, batch_size=128, epochs=50, verbose=1)\n",
    "print(\"  \")\n",
    "print(\"-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement your own analogy function\n",
    "skipgramweights1 = skipgram1.get_weights()\n",
    "cbowweights1 = cbow1.get_weights()\n",
    "\n",
    "skipgramweights2 = skipgram2.get_weights()\n",
    "cbowweights2 = cbow2.get_weights()\n",
    "\n",
    "skipgramweights3 = skipgram3.get_weights()\n",
    "cbowweights3 = cbow3.get_weights()\n",
    "\n",
    "skip_embedding1 = skipgramweights1[0]\n",
    "cbow_embedding1 = cbowweights1[1].transpose()\n",
    "skip_embedding2 = skipgramweights2[0]\n",
    "cbow_embedding2 = cbowweights2[1].transpose()\n",
    "skip_embedding3 = skipgramweights3[0]\n",
    "cbow_embedding3 = cbowweights3[1].transpose()\n",
    "\n",
    "def embed(word, embedding, tokenizer=tokenizer):\n",
    "    #get the index of the word from the tokenozer\n",
    "    int_word = tokenizer.texts_to_sequences([word])[0]\n",
    "    #get the size of the dictionary from the embedding matrix\n",
    "    dict_size = embedding.shape[0]\n",
    "    #print(embedding.shape)\n",
    "    #get the one-hot encoding of the word\n",
    "    bin_word = np_utils.to_categorical(int_word, dict_size)\n",
    "    return np.dot(bin_word, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "King - Man + Woman\n",
      "Similarity between resulting vector and 'Queen' vector:\n",
      "Skipgram, embedding vector length = 50: 0.32257384\n",
      "CBOW, embedding vector length = 50: 0.9532818\n",
      "Skipgram, embedding vector length = 150: 0.28733605\n",
      "CBOW, embedding vector length = 150: 0.9194461\n",
      "Skipgram, embedding vector length = 300: 0.06498463\n",
      "CBOW, embedding vector length = 300: 0.9359321\n",
      "\n",
      "Girl - Woman + Man\n",
      "Similarity between resulting vector and 'Boy' vector:\n",
      "Skipgram, embedding vector length = 50: 0.21181798\n",
      "CBOW, embedding vector length = 50: 0.94287366\n",
      "Skipgram, embedding vector length = 150: 0.074067\n",
      "CBOW, embedding vector length = 150: 0.90878516\n",
      "Skipgram, embedding vector length = 300: 0.14926662\n",
      "CBOW, embedding vector length = 300: 0.9161242\n",
      "\n",
      "moon - night + day\n",
      "Similarity between resulting vector and 'Sun' vector:\n",
      "Skipgram, embedding vector length = 50: 0.46686453\n",
      "CBOW, embedding vector length = 50: 0.96896935\n",
      "Skipgram, embedding vector length = 150: 0.32211307\n",
      "CBOW, embedding vector length = 150: 0.9288055\n",
      "Skipgram, embedding vector length = 300: 0.27272487\n",
      "CBOW, embedding vector length = 300: 0.9456606\n"
     ]
    }
   ],
   "source": [
    "print(\"King - Man + Woman\")\n",
    "print(\"Similarity between resulting vector and 'Queen' vector:\")\n",
    "queen_skip1 = embed('king', skip_embedding1) - embed('man', skip_embedding1) + embed('woman', skip_embedding1)\n",
    "queen_cbow1 = embed('king', cbow_embedding1) - embed('man', cbow_embedding1) + embed('woman', cbow_embedding1)\n",
    "dist = cosine_similarity(queen_skip1, embed('queen', skip_embedding1))\n",
    "print(\"Skipgram, embedding vector length = 50:\", str(dist[0][0]))\n",
    "\n",
    "dist = cosine_similarity(queen_cbow1, embed('queen', cbow_embedding1))\n",
    "print(\"CBOW, embedding vector length = 50:\", str(dist[0][0]))\n",
    "\n",
    "queen_skip2 = embed('king', skip_embedding2) - embed('man', skip_embedding2) + embed('woman', skip_embedding2)\n",
    "queen_cbow2 = embed('king', cbow_embedding2) - embed('man', cbow_embedding2) + embed('woman', cbow_embedding2)\n",
    "dist = cosine_similarity(queen_skip2, embed('queen', skip_embedding2))\n",
    "print(\"Skipgram, embedding vector length = 150:\", str(dist[0][0]))\n",
    "\n",
    "dist = cosine_similarity(queen_cbow2, embed('queen', cbow_embedding2))\n",
    "print(\"CBOW, embedding vector length = 150:\", str(dist[0][0]))\n",
    "\n",
    "queen_skip3 = embed('king', skip_embedding3) - embed('man', skip_embedding3) + embed('woman', skip_embedding3)\n",
    "queen_cbow3 = embed('king', cbow_embedding3) - embed('man', cbow_embedding3) + embed('woman', cbow_embedding3)\n",
    "dist = cosine_similarity(queen_skip3, embed('queen', skip_embedding3))\n",
    "print(\"Skipgram, embedding vector length = 300:\", str(dist[0][0]))\n",
    "\n",
    "dist = cosine_similarity(queen_cbow3, embed('queen', cbow_embedding3))\n",
    "print(\"CBOW, embedding vector length = 300:\", str(dist[0][0]))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Girl - Woman + Man\")\n",
    "print(\"Similarity between resulting vector and 'Boy' vector:\")\n",
    "boy_skip1 = embed('girl', skip_embedding1) - embed('woman', skip_embedding1) + embed('man', skip_embedding1)\n",
    "boy_cbow1 = embed('girl', cbow_embedding1) - embed('woman', cbow_embedding1) + embed('man', cbow_embedding1)\n",
    "\n",
    "dist = cosine_similarity(boy_skip1, embed('boy', skip_embedding1))\n",
    "print(\"Skipgram, embedding vector length = 50:\", str(dist[0][0]))\n",
    "\n",
    "dist = cosine_similarity(boy_cbow1, embed('boy', cbow_embedding1))\n",
    "print(\"CBOW, embedding vector length = 50:\", str(dist[0][0]))\n",
    "\n",
    "boy_skip2 = embed('girl', skip_embedding2) - embed('woman', skip_embedding2) + embed('man', skip_embedding2)\n",
    "boy_cbow2 = embed('girl', cbow_embedding2) - embed('woman', cbow_embedding2) + embed('man', cbow_embedding2)\n",
    "\n",
    "dist = cosine_similarity(boy_skip2, embed('boy', skip_embedding2))\n",
    "print(\"Skipgram, embedding vector length = 150:\", str(dist[0][0]))\n",
    "\n",
    "dist = cosine_similarity(boy_cbow2, embed('boy', cbow_embedding2))\n",
    "print(\"CBOW, embedding vector length = 150:\", str(dist[0][0]))\n",
    "\n",
    "boy_skip3 = embed('girl', skip_embedding3) - embed('woman', skip_embedding3) + embed('man', skip_embedding3)\n",
    "boy_cbow3 = embed('girl', cbow_embedding3) - embed('woman', cbow_embedding3) + embed('man', cbow_embedding3)\n",
    "\n",
    "dist = cosine_similarity(boy_skip3, embed('boy', skip_embedding3))\n",
    "print(\"Skipgram, embedding vector length = 300:\", str(dist[0][0]))\n",
    "\n",
    "dist = cosine_similarity(boy_cbow3, embed('boy', cbow_embedding3))\n",
    "print(\"CBOW, embedding vector length = 300:\", str(dist[0][0]))\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"moon - night + day\")\n",
    "print(\"Similarity between resulting vector and 'Sun' vector:\")\n",
    "\n",
    "sun_skip1 = embed('moon', skip_embedding1) - embed('night', skip_embedding1) + embed('day', skip_embedding1)\n",
    "sun_cbow1 = embed('moon', cbow_embedding1) - embed('night', cbow_embedding1) + embed('day', cbow_embedding1)\n",
    "\n",
    "dist = cosine_similarity(sun_skip1, embed('sun', skip_embedding1))\n",
    "print(\"Skipgram, embedding vector length = 50:\", str(dist[0][0]))\n",
    "\n",
    "dist = cosine_similarity(sun_cbow1, embed('sun', cbow_embedding1))\n",
    "print(\"CBOW, embedding vector length = 50:\", str(dist[0][0]))\n",
    "\n",
    "sun_skip2 = embed('moon', skip_embedding2) - embed('night', skip_embedding2) + embed('day', skip_embedding2)\n",
    "sun_cbow2 = embed('moon', cbow_embedding2) - embed('night', cbow_embedding2) + embed('day', cbow_embedding2)\n",
    "\n",
    "dist = cosine_similarity(sun_skip2, embed('sun', skip_embedding2))\n",
    "print(\"Skipgram, embedding vector length = 150:\", str(dist[0][0]))\n",
    "\n",
    "dist = cosine_similarity(sun_cbow2, embed('sun', cbow_embedding2))\n",
    "print(\"CBOW, embedding vector length = 150:\", str(dist[0][0]))\n",
    "\n",
    "sun_skip3 = embed('moon', skip_embedding3) - embed('night', skip_embedding3) + embed('day', skip_embedding3)\n",
    "sun_cbow3 = embed('moon', cbow_embedding3) - embed('night', cbow_embedding3) + embed('day', cbow_embedding3)\n",
    "\n",
    "dist = cosine_similarity(sun_skip3, embed('sun', skip_embedding3))\n",
    "print(\"Skipgram, embedding vector length = 300:\", str(dist[0][0]))\n",
    "\n",
    "dist = cosine_similarity(sun_cbow3, embed('sun', cbow_embedding3))\n",
    "print(\"CBOW, embedding vector length = 300:\", str(dist[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison performance: \n",
    "In the analogies presented above, we expect the cosine similarity to be as close as possible to one. We can cleraly see that the CBOW mdel is performing better than Skipgram as we expected, since Skipgram needs a bigger amount of training data to learn a good word embedding, while CBOW is faster and performs better on frequent words. In general, we could notice that the training of both models was quite poor and slow, even after increasing the number of epochs and tweaking the parameter, the loss is not changing anymore after a few steps. In conslusion, to fully take advantage of these two model and learn proper word embedding a larger and more diverse dataset is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "responsive": true,
        "showLink": false
       },
       "data": [
        {
         "hoverinfo": "text",
         "marker": {
          "opacity": 0.8,
          "size": 5
         },
         "mode": "markers+text",
         "text": [
          "alice",
          "rabbit",
          "fish",
          "lobster",
          "english",
          "man",
          "flamingo",
          "girl",
          "mice",
          "french",
          "duck",
          "ann",
          "twenty",
          "dogs",
          "below",
          "above",
          "boy",
          "tortoise",
          "unhappy",
          "woman",
          "cucumber",
          "apples",
          "daisy",
          "daisies",
          "apple",
          "rats",
          "magpie",
          "canary",
          "kill",
          "murder",
          "elsie",
          "lacie",
          "tillie",
          "kings",
          "queens",
          "fifteenth",
          "sixteenth",
          "sorrows",
          "joys"
         ],
         "textposition": "bottom center",
         "type": "scatter",
         "uid": "30cbd35f-dee8-44a8-b72d-7ecc31e43041",
         "x": [
          274.03179931640625,
          100.9393539428711,
          128.67494201660156,
          -186.16188049316406,
          -190.9329071044922,
          -120.20196533203125,
          -17.5305118560791,
          -214.49124145507812,
          -133.10000610351562,
          -66.47786712646484,
          -266.168212890625,
          243.39981079101562,
          84.48307037353516,
          206.85391235351562,
          11.416735649108887,
          54.22673416137695,
          -83.59292602539062,
          260.1817321777344,
          153.33847045898438,
          -129.46878051757812,
          152.5755157470703,
          -7.441366195678711,
          27.65602684020996,
          19.422832489013672,
          190.5625762939453,
          -35.64842224121094,
          -89.21563720703125,
          56.44611740112305,
          160.3704376220703,
          -177.63397216796875,
          123.1114501953125,
          90.98483276367188,
          1.7249079942703247,
          87.39846801757812,
          62.52893829345703,
          -20.360702514648438,
          9.601699829101562,
          -48.09929656982422,
          -103.49634552001953
         ],
         "y": [
          -109.45902252197266,
          -228.91720581054688,
          214.0111083984375,
          -64.21418762207031,
          -156.13197326660156,
          -122.6451187133789,
          169.7227783203125,
          16.502220153808594,
          -238.02207946777344,
          -190.52593994140625,
          -48.93190002441406,
          193.023681640625,
          -139.02806091308594,
          -30.761104583740234,
          -164.97801208496094,
          189.03811645507812,
          -55.9500732421875,
          61.6801643371582,
          -99.74304962158203,
          7.14915657043457,
          38.9505500793457,
          255.7406768798828,
          102.77301788330078,
          37.85504150390625,
          -189.8179168701172,
          74.82559204101562,
          155.25827026367188,
          -69.84832763671875,
          109.62933349609375,
          123.97673797607422,
          -23.537124633789062,
          118.74465942382812,
          -29.058090209960938,
          54.37864303588867,
          -6.701605796813965,
          -97.91799926757812,
          -260.363525390625,
          11.544655799865723,
          81.1635971069336
         ]
        }
       ],
       "layout": {
        "title": {
         "text": "Alice"
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"131ebde5-255e-4541-8302-7055df187876\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
       "                    \n",
       "                if (document.getElementById(\"131ebde5-255e-4541-8302-7055df187876\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '131ebde5-255e-4541-8302-7055df187876',\n",
       "                        [{\"hoverinfo\": \"text\", \"marker\": {\"opacity\": 0.8, \"size\": 5}, \"mode\": \"markers+text\", \"text\": [\"alice\", \"rabbit\", \"fish\", \"lobster\", \"english\", \"man\", \"flamingo\", \"girl\", \"mice\", \"french\", \"duck\", \"ann\", \"twenty\", \"dogs\", \"below\", \"above\", \"boy\", \"tortoise\", \"unhappy\", \"woman\", \"cucumber\", \"apples\", \"daisy\", \"daisies\", \"apple\", \"rats\", \"magpie\", \"canary\", \"kill\", \"murder\", \"elsie\", \"lacie\", \"tillie\", \"kings\", \"queens\", \"fifteenth\", \"sixteenth\", \"sorrows\", \"joys\"], \"textposition\": \"bottom center\", \"type\": \"scatter\", \"uid\": \"828defc5-5015-4809-a19b-3d403c2da9d9\", \"x\": [274.03179931640625, 100.9393539428711, 128.67494201660156, -186.16188049316406, -190.9329071044922, -120.20196533203125, -17.5305118560791, -214.49124145507812, -133.10000610351562, -66.47786712646484, -266.168212890625, 243.39981079101562, 84.48307037353516, 206.85391235351562, 11.416735649108887, 54.22673416137695, -83.59292602539062, 260.1817321777344, 153.33847045898438, -129.46878051757812, 152.5755157470703, -7.441366195678711, 27.65602684020996, 19.422832489013672, 190.5625762939453, -35.64842224121094, -89.21563720703125, 56.44611740112305, 160.3704376220703, -177.63397216796875, 123.1114501953125, 90.98483276367188, 1.7249079942703247, 87.39846801757812, 62.52893829345703, -20.360702514648438, 9.601699829101562, -48.09929656982422, -103.49634552001953], \"y\": [-109.45902252197266, -228.91720581054688, 214.0111083984375, -64.21418762207031, -156.13197326660156, -122.6451187133789, 169.7227783203125, 16.502220153808594, -238.02207946777344, -190.52593994140625, -48.93190002441406, 193.023681640625, -139.02806091308594, -30.761104583740234, -164.97801208496094, 189.03811645507812, -55.9500732421875, 61.6801643371582, -99.74304962158203, 7.14915657043457, 38.9505500793457, 255.7406768798828, 102.77301788330078, 37.85504150390625, -189.8179168701172, 74.82559204101562, 155.25827026367188, -69.84832763671875, 109.62933349609375, 123.97673797607422, -23.537124633789062, 118.74465942382812, -29.058090209960938, 54.37864303588867, -6.701605796813965, -97.91799926757812, -260.363525390625, 11.544655799865723, 81.1635971069336]}],\n",
       "                        {\"title\": {\"text\": \"Alice\"}},\n",
       "                        {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('131ebde5-255e-4541-8302-7055df187876');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualization results trained word embeddings\n",
    "wordlist=[]\n",
    "w=['sorrows',\n",
    "'joys',\n",
    "'unhappy',\n",
    "'dogs',\n",
    "'rabbit',\n",
    "'apple',\n",
    "'apples',\n",
    "'cucumber',\n",
    "'above',\n",
    "'below',\n",
    "'daisy',\n",
    "'daisies',\n",
    "'alice',\n",
    "'lacie',\n",
    "'tillie',\n",
    "'ann',\n",
    "'elsie',\n",
    "'rats',\n",
    "'flamingo',\n",
    "'duck',\n",
    "'fish',\n",
    "'kill',\n",
    "'murder',\n",
    "'english',\n",
    "'kings',\n",
    "'french',\n",
    "'girl',\n",
    "'boy',\n",
    "'man',\n",
    "'queens',\n",
    "'twenty',\n",
    "'woman',\n",
    "'tortoise',\n",
    "'mice',\n",
    "'magpie',\n",
    "'canary',\n",
    "'sixteenth',\n",
    "'fifteenth',\n",
    "'lobster']\n",
    "\n",
    "\n",
    "def get_coordinates(words, embedding):\n",
    "    arr = np.empty((0,embedding.shape[1]), dtype='f')\n",
    "    labels = []\n",
    "    for word in words:\n",
    "        if word in w:\n",
    "            wordlist.append(word)\n",
    "            try:\n",
    "                word_vector = embedding[words[word]]\n",
    "                arr = np.append(arr, np.array([word_vector]), axis=0)\n",
    "                labels.append(words[word])\n",
    "            except:\n",
    "                pass\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    return x_coords, y_coords\n",
    "\n",
    "x, y = get_coordinates(tokenizer.word_index, cbow_embedding1)\n",
    "\n",
    "plot = [go.Scatter(x = x,\n",
    "                    y = y,\n",
    "                    mode = 'markers+text',\n",
    "                    text = wordlist,\n",
    "                    textposition='bottom center',\n",
    "                    hoverinfo = 'text',\n",
    "                    marker=dict(size=5,opacity=0.8))]\n",
    "\n",
    "layout = go.Layout(title='Alice')\n",
    "fig = go.Figure(data=plot, layout=layout)\n",
    "plt.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation results of the visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our scenario, the training dataset is composed by one document only, the book of Alice in Wonderland, therefore the models don't have a sufficient representative sample from which they can learn a high quality word embedding. Additionally, since the book is a fictional novel, it does not provide enough diverse context and it does not represent properly the real word and consequently the real meaning of certain words. As we can see from the visualization, the model could learn the similairy between certain popular words in the book, like \"Queen\" and  \"King\", and \"Man\", \"Woman\" and \"Boy\", but it does not have sufficient information to learn the right representation of words like \"unhappy\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of the advantages of CBOW and Skipgram, the advantages of negative sampling and drawbacks of CBOW and Skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADVANTAGES:\n",
    "\n",
    "Though CBOW (predict target from context) and skip-gram (predict context words from target) are just inverted methods to each other, they each have their advantages/disadvantage:\n",
    "\n",
    "Skip Gram is found to represent rare words well. On the other hand, CBOW is faster and has better representations for more frequent words. In CBOW the vectors from the context words are averaged before predicting the center word. In skip-gram there is no averaging of embedding vectors. It seems like the model can learn better representations for the rare words when their vectors are not averaged with the other context words in the process of making the predictions.\n",
    "\n",
    "CBOW tends to produce vectors that are more topically related, because of how the algorithm works word order doesn’t have any influence on the target vectors. \n",
    "\n",
    "Since CBOW can use many context words to predict the one target word, it can essentially smooth out over the distribution. This is essentially like regularization and is offer very good performance when our input data is not so large. \n",
    "\n",
    "However the skip-gram model is more fine grained so we are able to extract more information and essentially have more accurate embeddings when we have a large data set (large data is always the best regularizer).\n",
    "\n",
    "Skip-gram pays more attention to words in closer proximity and tend to have more syntactic information as a result. \n",
    "\n",
    "These are low on memory. They don’t need to have huge RAM requirements like that of co-occurrence matrix where it needs to store three huge matrices.\n",
    "\n",
    "\n",
    "DISADVANTAGES:\n",
    "\n",
    "As the size of the vocabulary of the corpus grows, these models get computationally more expensive due to the softmax layer. To calculate the probability everytime, the softmax layer needs to carry out a sum over all the words in the vocabulary.\n",
    "\n",
    "Both of the CBOW and Skipgram model fails to identify the combined word phrases. e.g “New York” is a single word and cannot be treated as New and York two different words.\n",
    "\n",
    "CBOW and Skipgram cannot capture the polysemy (one word with different meanings), because they tend to represent a word as a single vector.\n",
    "\n",
    "ADVANTAGES OF NEGATIVE SAMPLING:\n",
    "Negative sampling use the concept of noise distribution to distinguish positive sample from the negative ones, \n",
    "since a good model should be able to differentiate fake signal from the real data. Also, we know that training a neural network means looking at a training dataset and adjusting all of the weights so that it predicts that training sample more accurately. In other words, each training sample will tweak all of the weights in the neural network. Connsidering the size of the vocabulary, the skip-gram and cbow models will have an huge number of weights, all of which would be updated slightly by every one of our training samples. Negative sampling increases computational efficiency by having each training sample only modify a small percentage of the weights, rather than all of them, since it uses only K samples at time instead of considering the whole vocabulary.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEER REVIEW\n",
    "\n",
    "Work was equally divided between the two group members. Equal contribution was made, \n",
    "in both implementation and conceptual writing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
